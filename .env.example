# Employment Act Malaysia Compliance Agent - Environment Configuration
# Copy this to .env and update values for your deployment

# ===== MODEL CONFIGURATION =====
# Base model for vLLM serving (use an Instruct model by default)
MODEL_NAME=Qwen/Qwen2.5-1.5B-Instruct

# LoRA adapter path (optional - leave empty for base model only)
ADAPTER_PATH=

# Alternative: Use merged model (if you've merged LoRA weights)
# MODEL_NAME=./outputs/merged_model

# Hugging Face token for model access (optional)
HF_TOKEN=your_hf_token_here

# ===== INFERENCE CONFIGURATION =====
# vLLM server settings
VLLM_BASE_URL=http://localhost:8000
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
MAX_MODEL_LEN=4096
GPU_MEMORY_UTIL=0.9
MAX_NUM_BATCHED_TOKENS=2048
TENSOR_PARALLEL_SIZE=1

# Generation parameters
MAX_TOKENS=512
TEMPERATURE=0.0
TOP_K=6
VLLM_TIMEOUT=60.0
CIRCUIT_BREAKER_THRESHOLD=5

# Quantization (optional: awq, gptq, squeezellm)
# QUANTIZATION=awq

# ===== RETRIEVAL CONFIGURATION =====
# Hour 2 spec: BM25 top-100 ∪ dense top-50 → CE → top-8
# Optimized defaults for performance (can override for spec compliance)
BM25_TOPK=30
DENSE_TOPK=20
CE_MAX_PAIRS=40
FINAL_TOPK=8
MIN_CHUNKS=6

# Cross-encoder batching for performance
CE_BATCH_SIZE=16

# Advanced rewrite toggle
USE_ADVANCED_REWRITE=false

# ===== API SERVER CONFIGURATION =====
API_HOST=0.0.0.0
API_PORT=8001
API_WORKERS=1
API_RELOAD=false
LOG_LEVEL=info

# CORS settings
ALLOWED_ORIGINS=http://localhost:8501,http://localhost:3000

# ===== UI CONFIGURATION =====
STREAMLIT_HOST=0.0.0.0
STREAMLIT_PORT=8501
API_BASE_URL=http://localhost:8001

# ===== DATA PATHS =====
# RAG system paths
FAISS_INDEX_PATH=data/indices/faiss.index
STORE_PATH=data/indices/store.pkl
RAW_PDFS_PATH=data/raw_pdfs
PROCESSED_DATA_PATH=data/processed
EVAL_DATA_PATH=data/eval

# Embedding and reranking models (aligned with code defaults)
EMBEDDING_MODEL=BAAI/bge-m3
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-2-v2

# Retrieval filtering
MIN_CONTEXT_SCORE=0.15

# ===== GUARDRAILS CONFIGURATION =====
# Guardrails config path (optional - falls back to config/guardrails.yaml)
GUARDRAILS_CONFIG=

# ===== CACHING CONFIGURATION =====
# Redis for distributed caching (optional - uses in-memory only if not set)
REDIS_URL=

# Cache settings
CACHE_MEMORY_SIZE=1000
CACHE_TTL=1800

# ===== MONITORING & OBSERVABILITY =====
# Logging
LOG_LEVEL=info

# Metrics
METRICS_ENABLED=true

# OpenTelemetry (optional)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# OTEL_SERVICE_NAME=employment-act-api
# OTEL_RESOURCE_ATTRIBUTES=service.version=1.0.0

# ===== SECURITY =====
# API authentication (optional)
# API_AUTH_TOKEN=your-secret-token

# ===== DEPLOYMENT =====
# Git SHA for version tracking
GIT_SHA=development

# Environment
ENVIRONMENT=development

# ===== EVALUATION DATASETS =====
RETRIEVAL_GOLD_PATH=data/eval/retrieval_gold.jsonl
QA_GOLD_PATH=data/eval/qa_gold.jsonl
REFUSAL_GOLD_PATH=data/eval/refusal_gold.jsonl

# ===== TRAINING PATHS (if using training pipeline) =====
SFT_OUTPUT_PATH=outputs/lora_sft
DPO_OUTPUT_PATH=outputs/lora_dpo
PPO_OUTPUT_PATH=outputs/lora_ppo

# Legacy model names (for backwards compatibility)
BASE_MODEL=meta-llama/Llama-3.1-8B-Instruct
CHUNK_SIZE=1000
CHUNK_STRIDE=300
