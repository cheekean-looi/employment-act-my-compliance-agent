# Development Complete Pipeline Configuration
# Usage: python run_complete_pipeline.py dev --config config/development.yaml

# Input configuration
input_path: "data/raw_pdfs"
output_dir: "outputs/dev"
experiment_name: "dev_test"

# Data pipeline configuration (small for fast iteration)
data_config:
  pdf_limit: 3           # Only process 3 PDFs
  chunk_size: 800        # Smaller chunks
  chunk_stride: 100
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  force_rebuild: false

# SFT pipeline configuration (fast settings)
sft_config:
  dataset_size: 50        # Small dataset for quick testing
  epochs: 1               # Single epoch for speed
  learning_rate: 2e-4     # Higher LR for faster convergence
  batch_size: 2           # Smaller batch for memory
  # Use a public, lightweight model for Mac/MPS development
  model_name: "HuggingFaceTB/SmolLM-135M-Instruct"
  use_sfttrainer: true    # TRL trainer for simplicity
  bf16: false             # Disable bf16 on MPS/CPU
  use_4bit: false         # Disable 4-bit quantization off CUDA

# RLAIF pipeline configuration (minimal for testing)
rlaif_config:
  pairs_size: 20         # Few pairs for quick testing
  dpo_epochs: 1          # Single epoch
  dpo_beta: 0.3          # Higher beta for stability
  dpo_learning_rate: 1e-4
  ppo_prompts: 8         # Minimal prompts
  ppo_model: "HuggingFaceTB/SmolLM-135M-Instruct"

# Pipeline control
skip_data: false
skip_sft: false
skip_rlaif: false
skip_evaluation: false

# Resource management (memory conservative)
max_memory_gb: 16
gpu_memory_fraction: 0.7
cleanup_intermediate: false  # Keep files for debugging

# Advanced options
dry_run: false
verbose: true
logging_backends: ["tensorboard"]  # Local logging only
